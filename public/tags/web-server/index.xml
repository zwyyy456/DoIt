<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Web Server on My New Hugo Site</title>
    <link>http://localhost:1313/tags/web-server/</link>
    <description>Recent content in Web Server on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Nov 2023 13:00:39 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/web-server/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Web Day12：实现主从 Reactor 多线程模式</title>
      <link>http://localhost:1313/posts/tech/web-day12.zh/</link>
      <pubDate>Sun, 19 Nov 2023 13:00:39 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day12.zh/</guid>
      <description>前言 在 Day11 中，我们实现了一种最容易想到的 Reactor 多线程模式，即将每个 Channel 的任务分配给一个线程执行。&#xA;这个模式逻辑上有不少问题，例如线程池由 EventLoop 来持有，按理来说应该由 Server 类来管理，这是受到了 Channel 类的限制，Channel 类仅有 EventLoop 成员。&#xA;主从 Reactor 模式 主从 Reactor 模式有以下几个特点：&#xA;服务器一般只有一个 main reactor，可以有很多个 sub reactor； 服务器管理一个线程池，每个线程对应一个 sub reactor，每个 sub reactor 负责一部分 Connections 的事件循环，事件执行也在这个线程完成； main reactor 只负责 Acceptor 建立新连接，然后将这个连接分配给一个 sub Reactor。 Server 成员 /todo，测试 accept 和 connect 的时候区分非阻塞与阻塞&#xA;Server 的成员包括一个 main_reactor 和 多个 sub_reactors，每个 sub_reactor 对应一个独有的 EventLoop，每个 sub_reactor 由一个线程负责。这就是所谓的 One Loop per Thread。&#xA;class Server { private: EventLoop *main_reactor_; Acceptor *acceptor_; std::map&amp;lt;int, Connection *&amp;gt; connections_; std::vector&amp;lt;EventLoop *&amp;gt; sub_reactors_; ThreadPool *thpool_; public: Server(EventLoop *evl); ~Server(); void HandleReadEvent(int fd); void NewConn(Socket *serv_sock); void DeleteConn(int sockfd); }; main reactor 的工作流程 Server 创建的时候，会利用 main 函数的 loop 来初始化 Server，并利用 loop 来初始化 main_reactor_，和 acceptor_。 acceptor_ 会有绑定了服务器 ip 和端口的 socket。初始化 acceptor_ 的时候，会将 acceptor_-&amp;gt;new_conn_callback_ 注册为 Server::NewConn(Socket *clnt_sock)，当有连接时，acceptor_ 调用 Acceptor::AcceptConn()，该函数会调用 Socket::Accept(InetAddress *) 来接受连接，以及调用 new_conn_callback_(clnt_sock)，实际上就是调用 Server::NewConn(Socket *clnt_sock)。</description>
    </item>
    <item>
      <title>Web Day11：完成线程池以及加入一个简单的测试程序</title>
      <link>http://localhost:1313/posts/tech/web-day11.zh/</link>
      <pubDate>Sat, 18 Nov 2023 19:40:26 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day11.zh/</guid>
      <description>需要完善的地方 Day10 中，我们添加了一个简单的线程池，一个完整的 Reactor 模型已经成型。但这个线程池存在的问题还比较多，例如任务队列的取出、添加都存在拷贝，性能较差，只能用于学习。&#xA;正确操作应该使用右值移动、完美转发等来阻止拷贝。&#xA;另外，线程池只能接受 std::function&amp;lt;void()&amp;gt; 类型的函数，所以函数需要使用 Lambda 表达式来创建，或者 std::bind()，且无法得到返回值。&#xA;利用模板 class ThreadPool { private: std::vector&amp;lt;std::thread&amp;gt; threads_; std::queue&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; tasks_; std::mutex tasks_mtx_; std::condition_variable cv_; bool stop_; public: explicit ThreadPool(int size = 8); ~ThreadPool(); template &amp;lt;class F, class... Args&amp;gt; auto add_task(F &amp;amp;&amp;amp;f, Args &amp;amp;&amp;amp;...args) -&amp;gt; std::future&amp;lt;typename std::result_of&amp;lt;F(Args...)&amp;gt;::type&amp;gt;; }; template &amp;lt;class F, class... Args&amp;gt; auto ThreadPool::add_task(F &amp;amp;&amp;amp;f, Args &amp;amp;&amp;amp;...args) -&amp;gt; std::future&amp;lt;typename std::result_of&amp;lt;F(Args...)&amp;gt;::type&amp;gt; { using return_type = typename std::result_of&amp;lt;F(Args...)&amp;gt;::type; auto ptask = std::make_shared&amp;lt;std::packaged_task&amp;lt;return_type()&amp;gt;&amp;gt;( std::bind(std::forward&amp;lt;F&amp;gt;(f), std::forward&amp;lt;Args&amp;gt;(args).</description>
    </item>
    <item>
      <title>Web Day10：加入线程池到服务器</title>
      <link>http://localhost:1313/posts/tech/web-day10.zh/</link>
      <pubDate>Sat, 18 Nov 2023 15:19:11 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day10.zh/</guid>
      <description>前言 到 day9 的时候，一个单线程的服务器已经算写好了。Reactor 驱动大致成型。&#xA;服务器的启动流程大致如下，先创建 EventLoop 对象 loop（里面包含了 Epoll 对象），然后 Server 会利用 loop 实例化对象 server，Server 对象实例化时，Acceptor 类型的 acceptor_ 对象会被初始化, Acceptor 对象用于建立连接，会在 Server 的构造函数里回调函数 acceptor_-&amp;gt;new_conn_callback_ 会被注册为 server-&amp;gt;NewConn(Socket *clnt_sock) ，由 acceptor_-&amp;gt;AcceptConn() 来调用。而 Acceptor 的构造函数中 Channel 类的实例 accept_ch_ 会被初始化，而回调函数 acceptr_ch-&amp;gt;callback_ 会被注册为 acceptor_-&amp;gt;AcceptConn()，最终还是调用的 server-&amp;gt;NewConn(Socket *clnt_sock)。&#xA;Server::Server(EventLoop *loop) : loop_(loop), acceptor_(nullptr) { acceptor_ = new Acceptor(loop); std::function&amp;lt;void(Socket *)&amp;gt; callback = [this](auto &amp;amp;&amp;amp;PH1) { NewConn(std::forward&amp;lt;decltype(PH1)&amp;gt;(PH1)); }; acceptor_-&amp;gt;set_new_conn_callback(callback); } void Server::NewConn(Socket *sock) { auto *conn = new Connection(loop_, sock); // 这里应该是 clnt_sock std::function&amp;lt;void(Socket *)&amp;gt; callback = [this](auto &amp;amp;&amp;amp;PH1) { DeleteConn(std::forward&amp;lt;decltype(PH1)&amp;gt;(PH1)); }; conn-&amp;gt;set_delete_conn_callback(callback); connections_[sock-&amp;gt;getfd()] = conn; } void Acceptor::AcceptConn() { auto *clnt_addr = new InetAddress(); auto *clnt_sock = new Socket(sock_-&amp;gt;Accpet(clnt_addr)); printf(&amp;#34;new client fd %d!</description>
    </item>
    <item>
      <title>Web Day9：建立读写缓冲类</title>
      <link>http://localhost:1313/posts/tech/web-day9.zh/</link>
      <pubDate>Sat, 18 Nov 2023 14:53:43 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day9.zh/</guid>
      <description>概述 本节只是额外添加了一个读写的缓冲区，意义不甚明了。暂定。</description>
    </item>
    <item>
      <title>Web Day8：抽象出 Connection 类</title>
      <link>http://localhost:1313/posts/tech/web-day8.zh/</link>
      <pubDate>Thu, 16 Nov 2023 15:25:27 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day8.zh/</guid>
      <description>Acceptor 类 一言以蔽之，Acceptor 类负责接受连接，调用 AcceptConn，到这里，接受连接已经完全由 Acceptor 类来负责。Acceptconn 会调用被注册的回调函数 new_conn_callback_(clnt_sock)，实际上就是调用 Server::NewConn(clnt_sock)，该函数会将 {clnt_fd, conn} 的键值对添加到 Server 类的 map 中去。&#xA;此外，还会为 Connection 类注册删除 Connection 的回调函数；&#xA;void Acceptor::AcceptConn() { auto *clnt_addr = new InetAddress(); auto *clnt_sock = new Socket(sock_-&amp;gt;Accpet(clnt_addr)); printf(&amp;#34;new client fd %d! IP: %s Port: %d\n&amp;#34;, clnt_sock-&amp;gt;getfd(), inet_ntoa(clnt_addr-&amp;gt;get_addr().sin_addr), ntohs(clnt_addr-&amp;gt;get_addr().sin_port)); clnt_sock-&amp;gt;Setnonblocking(); new_conn_callback_(clnt_sock); delete clnt_addr; } void Server::NewConn(Socket *sock) { auto *conn = new Connection(loop_, sock); // 这里应该是 clnt_sock std::function&amp;lt;void(Socket *)&amp;gt; callback = [this](auto &amp;amp;&amp;amp;PH1) { DeleteConn(std::forward&amp;lt;decltype(PH1)&amp;gt;(PH1)); }; conn-&amp;gt;set_delete_conn_callback(callback); connections_[sock-&amp;gt;getfd()] = conn; Acceptor 的回调函数的注册过程发生在 Server 类的创建过程中。</description>
    </item>
    <item>
      <title>Web Day6：EventLoop 类与事件驱动</title>
      <link>http://localhost:1313/posts/tech/web-day6.zh/</link>
      <pubDate>Wed, 15 Nov 2023 20:09:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day6.zh/</guid>
      <description>事件驱动 原先的代码中，不管是接受客户端连接还是处理客户端事件，都是围绕epoll来编程，可以说epoll是整个程序的核心，服务器做的事情就是监听epoll上的事件，然后对不同事件类型进行不同的处理。这种以事件为核心的模式又叫事件驱动，事实上几乎所有的现代服务器都是事件驱动的。和传统的请求驱动模型有很大不同，事件的捕获、通信、处理和持久保留是解决方案的核心结构。&#xA;将服务器改造成 Reactor 模式 我们将服务器抽象成一个 Server 类，类中有一个 main-reactor，main-reactor 的核心是一个 EventLoop，即不断循环，一旦有事件发生，我们就会通过 ep-&amp;gt;Poll 知晓，然后作出对应的处理。&#xA;Channel 的修改 day5 中，每个 channel 里面都含有一个 Epoll * 指针，表示它在哪个 epoll 实例中被关注，这里我们把 Epoll * 替换成了 EventLoop *，表示该 channel 处于哪个事件循环中，事实上 EventLoop 的关键成员就是 Epoll *。&#xA;初始化流程 首先创建 EventLoop 对象 loop，它会创建一个 Epoll 实例，核心就是 epfd，然后利用 loop 初始化 Server 对象，在这个过程中，会完成服务器的 serv_fd 的创建以及 bind，listen，同时将对应着 serv_fd 以及新建连接事件的 Channel 创建出来，将 Channel 的处理事件的回调函数设置为 NewConn；调用 serv_ch-&amp;gt;EnableReading 会将关注的事件类型设置为 EPOLLIN | EPOLLET 并调用 loop-&amp;gt;UpdateChannel(this) 最终调用 ep-&amp;gt;UpdateChannel(ch)，从而将 serv_ch 对应的文件描述符添加到 epoll 关注的文件描述符列表或者修改。</description>
    </item>
    <item>
      <title>Web Day 5 添加 Channel 类</title>
      <link>http://localhost:1313/posts/tech/web-day5.zh/</link>
      <pubDate>Wed, 15 Nov 2023 15:29:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/web-day5.zh/</guid>
      <description>Channel 类 Channel 类相当于将 ep-&amp;gt;addFd 这一步拆成了两步，第一步是 ch-&amp;gt;enablereading，它会调用 ep-&amp;gt;UpdateChannel(this)，this 就是调用 enablereading 的那个 ch。&#xA;如何理解 Channel 类？可以认为每一个 ch 的实例，都对应着一个关注的文件描述符 fd 和一个要关注的事件类型 events，当前其实只有两类 Channel，一个是对应的服务器的 fd，另一类对应的是 accept 客户端的连接之后得到的 fd，ch-&amp;gt;events 是 fd 所在的 Channel 实例需要关注的事件类型。&#xA;而 active_events 表示该 Channel 当前发生的事件类型，在 ep-&amp;gt;Poll() 中会被设置。&#xA;auto Epoll::Poll(int timeout) -&amp;gt; std::vector&amp;lt;Channel *&amp;gt; { std::vector&amp;lt;Channel *&amp;gt; active_channels; int nfds = epoll_wait(epfd, events, MAX_EVENTS, timeout); errif(nfds == -1, &amp;#34;epoll wait error\n&amp;#34;); active_channels.reserve(nfds); for (int i = 0; i &amp;lt; nfds; ++i) { Channel *ch = (Channel *)events[i].</description>
    </item>
  </channel>
</rss>
