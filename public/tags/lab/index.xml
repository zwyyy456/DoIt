<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lab on My New Hugo Site</title>
    <link>http://localhost:1313/tags/lab/</link>
    <description>Recent content in Lab on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Aug 2023 13:53:24 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/lab/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Xv6 Lab11: Mmap</title>
      <link>http://localhost:1313/posts/tech/xv6-lab11.zh/</link>
      <pubDate>Thu, 03 Aug 2023 13:53:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab11.zh/</guid>
      <description>思路与实现 添加系统调用就不多说了。&#xA;整体流程应该是这样的，lab 的提示中，要求我们定义一个 vma 结构体，vma 的定义如下；然后 lab 的提示要求我们声明一个大小为 $16$ 的 vma 数组，并按需要从该数组分配，问题来了，数组在哪里声明呢？考虑到每个进程都有自己的虚拟地址空间，因此，每个进程都有自己的 virtual memory areas，要分配 vma 的时候，应该从每个进程自己的 vma 数组进行分配，于是，我们可以考虑为 struct proc 添加 struct vma areas[NVMA] 字段。&#xA;struct vma { int fd; int rw_flag; uint64 start; uint64 cur; uint len; int state; int flags; }; struct proc { // 已有的省略不写 struct vma areas[NVMA]; }; 在 vma 的定义中，start 表示起始地址，$[start, cur)$ 这一段虚拟地址（左闭右开）是已经绑定了 pp 的，pp 的数据与 file 绑定。&#xA;那么我们如何实现 sys_mmap 呢？这里可以参照 sbrk，递增 p-&amp;gt;sz，然后仿照 allocproc，寻找状态为 UNUSED 的 vma，分配给本次 sys_mmap。注意如果文件本身是 read_only，并且以 MAP_SHARED 模式进行 map，那么 flags 不能为 PROT_WRITE，write only 的情况同理（即文件不可读）。</description>
    </item>
    <item>
      <title>Xv6 Lab10: file system</title>
      <link>http://localhost:1313/posts/tech/xv6-lab10.zh/</link>
      <pubDate>Tue, 01 Aug 2023 20:19:47 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab10.zh/</guid>
      <description>Large files 这个作业需要我们将 xv6 的最大文件大小从 12 + 256 Bytes 修改为 11 + 256 + 256 * 256 Bytes。&#xA;为了达成这个目标，我们需要使用二级索引块，对 inode 的 addrs 字段，首先将 NDIRECT 从 $12$ 修改为 $11$，即前 $11$ 个 block 是 direct block，addrs[NDIRECT] 对应的块是一个一级索引块，这个块中的每个元素（共 BSIZE / sizeof(uint) 个元素）都是一个数据块的编号；而 addrs[NDIRECT + 1] 是一个二级索引块，这个块中的每个元素都是一级索引块的编号，由编号找到一级索引块，然后再由以及索引块找到数据块。&#xA;这个作业的主要任务就是修改 bmap 和 itrunc 两个函数。&#xA;先修改 fs.h 中的 NDIRECT 的相关定义：&#xA;#define FSMAGIC 0x10203040 #define NDIRECT 11 #define NINDIRECT (BSIZE / sizeof(uint)) #define NDINDIRECT (NINDIRECT * NINDIRECT) #define MAXFILE (NDIRECT + NINDIRECT + NDINDIRECT) struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT + 2]; // Data block addresses }; 注意 file.</description>
    </item>
    <item>
      <title>Xv6 Lab9: Locks</title>
      <link>http://localhost:1313/posts/tech/xv6-lab9.zh/</link>
      <pubDate>Sat, 29 Jul 2023 14:28:58 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab9.zh/</guid>
      <description>Memory allocator 这一题很简单，主要任务，就是为每个 cpu 维护一个空闲物理内存的链表 freelist，xv6 默认使用的结构体 kmem，其中包含一个 freelist 供所有的 cpu 使用。我们要做的，就是把 freelist 修改成 freelist 的数组，即 struct run *freelist[NCPU]，其中 NCPU 是定义于 kernel/params.h 的宏，对应 cpu 的个数。 kmem 中的 spinlock 是用来保护 freelist 的，既然 freelist 变成了数组，那么也需要有 NCPU 个 spinlock。因此，修改 kmem 为如下结构体：&#xA;struct { struct spinlock lock[NCPU]; struct run *freelist[NCPU]; // for each cpu, allocate a freelist } kmem; 接着，我们需要修改 kinit，让它初始化每个 spinlock。&#xA;void kinit() { char lockname[6] = {&amp;#39;k&amp;#39;, &amp;#39;m&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;m&amp;#39;, &amp;#39;0&amp;#39;, 0}; for (int i = 0; i &amp;lt; NCPU; ++i) { lockname[4] = &amp;#39;0&amp;#39; + i; initlock(kmem.</description>
    </item>
    <item>
      <title>Xv6 Lab7: Multithreading</title>
      <link>http://localhost:1313/posts/tech/xv6-lab7.zh/</link>
      <pubDate>Sat, 22 Jul 2023 11:30:01 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab7.zh/</guid>
      <description>Uthread: switching between threads 这个题还是对的起它 moderate 的难度了，如果认真看了 book-riscv-rev2.pdf 的 Scheduling 章节，以及看了这个 课程翻译，那么这题可以很快做出来，个人觉得 pdf 讲得更加清楚一些。&#xA;这个题甚至帮你把需要添加代码的地方都标注出来了，参照题目说明，主要有三步：&#xA;修改 thread_create 来保证当 thread_schedule 第一次运行 thread_create 创建出来的线程时，该线程就会在自己的 stack 上执行传递给 thread_create 的函数，这里我们可以参照 allocproc 的实现，在 thread_create 标记出来的要我们添加代码的地方添加如下三行： memset(&amp;amp;t-&amp;gt;context, 0, sizeof(t-&amp;gt;context)); t-&amp;gt;context.ra = (uint64)func; t-&amp;gt;context.sp = (uint64)t-&amp;gt;stack + STACK_SIZE; 保证 thread_switch 会切换并保存寄存器，这里参照 scheduler 的实现即可，在注释标记的地方添加以下语句，并且在 uthread_switch.S 中实现 thread_switch 函数（照抄 swtch 即可）： thread_switch((uint64)&amp;amp;t-&amp;gt;context, (uint64)&amp;amp;current_thread-&amp;gt;context); thread_switch: /* YOUR CODE HERE */ sd ra, 0(a0) sd sp, 8(a0) sd s0, 16(a0) sd s1, 24(a0) sd s2, 32(a0) sd s3, 40(a0) sd s4, 48(a0) sd s5, 56(a0) sd s6, 64(a0) sd s7, 72(a0) sd s8, 80(a0) sd s9, 88(a0) sd s10, 96(a0) sd s11, 104(a0) ld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1) ret /* return to ra */ 修改 strcut thread 来存储 thread_switch 时需要保存的寄存器，还是参照 struct proc 即可： struct t_context { uint64 ra; uint64 sp; // callee saved uint64 s0; uint64 s1; uint64 s2; uint64 s3; uint64 s4; uint64 s5; uint64 s6; uint64 s7; uint64 s8; uint64 s9; uint64 s10; uint64 s11; }; struct thread { char stack[STACK_SIZE]; /* the thread&amp;#39;s stack */ int state; /* FREE, RUNNING, RUNNABLE */ struct t_context context; }; 这样修改之后就能通过 uthread 了。</description>
    </item>
    <item>
      <title>Xv6 Lab6: Copy-on-Write Fork for xv6</title>
      <link>http://localhost:1313/posts/tech/xv6-lab6.zh/</link>
      <pubDate>Mon, 17 Jul 2023 13:42:27 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab6.zh/</guid>
      <description>思路 经过 lab5: lazy page allocation 之后，对 xv6 的 page fault 的处理，算是有所了解了。&#xA;今天这个 COW 实验，在 2020 年的课程视频中有对思路的讲解，可以先看看 课程翻译，厘清一下思路。&#xA;整体思路其实也不难，默认情况下，fork 会调用 uvmcopy，将父进程的 PP（物理页）复制一份，将这个 PP 的副本映射到子进程的 pagetable 的 VP（虚拟页）（子进程和父进程具有相同的虚拟地址，不同的 pagetable，不同的 PP，但是相同虚拟地址对应的 PP 的内容是一样的）。&#xA;我们这里讨论 vaddr、paddr 都是基于地址已经是 PGSIZE 对齐的情况来讨论的。&#xA;我们要做的修改就是，不再复制这个 PP，而是将 PP 的 paddr 同时映射到父进程的 vaddr 以及子进程的 vaddr。&#xA;在未修改 uvmcopy 之前，修改父进程的 vaddr 处的内容并不会影响子进程的 vaddr 处的内容，因为两个 vaddr 对应的是不同的 PP，只是 PP 的内容相同而已（在 pp 是 clean 的情况下）。&#xA;而修改了 uvmcopy 之后，写入父进程的 vaddr 会影响子进程的 vaddr 处的内容，这是我们不希望看到的，因此我们将这个 PP 对应的父进程的 pte 和子进程的 pte 的 PTE_W 位都清零，即标记为不可写，这样，当我们试图往这个 PP 中写入内容的时候，就会出现 page fault。</description>
    </item>
    <item>
      <title>Xv6 Lab5: lazy page allocation</title>
      <link>http://localhost:1313/posts/tech/xv6-lab5.zh/</link>
      <pubDate>Sat, 15 Jul 2023 17:18:30 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab5.zh/</guid>
      <description>前言 这个实验只有 2020 年的才有，2021 年的课程中是没有的，但是感觉这个实验还是挺有意义的，因此用 docker 创建了一个 debian 12 的容器，在容器中搭建了 2020 的实验环境，实验环境的搭建过程可以参照 MIT 6.s081 实验环境搭建。&#xA;Eliminate allocation from sbrk() 这个算是最简单的：&#xA;// kernel/sysproc.c uint64 sys_sbrk(void) { int addr; int n; if (argint(0, &amp;amp;n) &amp;lt; 0) { return -1; } addr = myproc-&amp;gt;sz; myproc()-&amp;gt;sz += n; // 添加的部分，修改 p-&amp;gt;sz，然后注释掉下面这三行 // if (growproc(n) &amp;lt; 0) { // return -1; // } return addr; } Lazy allocation 在去掉了 sys_sbrk 的 growproc 部分之后，由于只是单纯增加了 p-&amp;gt;sz，而没有给对应的虚拟地址分配物理页，因此，在执行 echo hi 时，会访问到 heap 中的未分配物理页的虚拟地址，于是出现 page fault，默认的 Xv6 的代码中并没有给出对 page fault 的处理，而是会直接杀死进程，因此无法正常执行完 echo hi。</description>
    </item>
    <item>
      <title>Xv6 Lab4: Traps</title>
      <link>http://localhost:1313/posts/tech/xv6-lab4.zh/</link>
      <pubDate>Thu, 13 Jul 2023 12:51:50 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab4.zh/</guid>
      <description>RISC-V assembly Which registers contain arguments to functions? For example, which register holds 13 in main&amp;rsquo;s call to printf?&#xA;a2 寄存器，函数调用时，参数从左到右会依次保存在 a0, a1, a2, a3 寄存器，似乎是一直到寄存器 a7 的。 Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)&#xA;这里的调用都被内联了 At what address is the function printf located?&#xA;auipc（Add Upper Immediate PC）指令是将一个立即数左移 $12$ 位加上当前指令的地址（pc）中，得到一个绝对地址。&#xA;例如 auipc a0, 0x0 就是将 $\text{0x0}$ 左移 $12$ 位加上当前指令的地址 (pc) 中（pc 的值我们一般认为是在指令执行完成时才发生递增，从而指向下一条指令，使得处理器能够按顺序顺利执行指令序列），因此，执行 auipc a0, 0x0 时，加的就是当前指令的地址，即 $\text{0x28}$。</description>
    </item>
    <item>
      <title>Xv6 Lab2: System calls</title>
      <link>http://localhost:1313/posts/tech/xv6-lab2.zh/</link>
      <pubDate>Sat, 01 Jul 2023 15:15:38 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab2.zh/</guid>
      <description>系统调用 Lab1 主要是基于提供的系统调用接口来编写一些小工具程序，而 Lab2 则是要我们自己实现系统调用，并提供系统调用的接口。&#xA;以本次 Lab 要我们实现的 trace 调用为例，说明一下系统调用的流程：&#xA;在 user/trace.c 的第 $15$ 行，调用了属于 system call 的 trace 函数，当前执行 make qemu 是无法成功的，因为我们还没有给用户提供接口。因此，我们需要在 user/user.h 里面添加系统调用 trace 的函数声明（prototype）。&#xA;我们需要在 user/usys.pl 中追加 entry(&amp;quot;trace&amp;quot;); 这一行，从而添加一个 trap entry，从而实现调用 trace 时会发生 trap，从而会执行 ECALL 指令，并且会将系统调用的接口的参数（这里就是 trace 的参数）的存入寄存器 a0、a1 等（从左往右第一个参数的地址存入 a0，依次类推），此外，还会将 trace 对应的系统调用号存入寄存器 a7。&#xA;之后控制权来到 kernel 中的 syscall 函数，它从 a7 中取出系统调用号，并执行系统调用号对应的 sys_func。&#xA;这里的系统调用号可以理解为数组索引，在本次 Lab 中需要我们修改 kernel/syscall.c 和 kenel/syscall.h 从而添加 trace 对应的系统调用号，以及内核中 trace 对应的 sys_trace 的实现）。&#xA;System call tracing 官网的提示其实是比较全面了，按提示处理，即可添加 trace 的系统调用接口：</description>
    </item>
  </channel>
</rss>
