<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mit on My New Hugo Site</title>
    <link>http://localhost:1313/tags/mit/</link>
    <description>Recent content in Mit on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/mit/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>计算机教育缺失的一课：Git</title>
      <link>http://localhost:1313/posts/tech/missing-semester-git.zh/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/tech/missing-semester-git.zh/</guid>
      <description>版本控制系统介绍 版本控制系统 (VCSs) 是一类用于追踪源代码（或其他文件、文件夹）改动的工具。顾名思义，这些工具可以帮助我们管理代码的修改历史；不仅如此，它还可以让协作编码变得更方便。VCS 通过一系列的快照将某个文件夹及其内容保存了起来，每个快照都包含了文件或文件夹的完整状态。同时它还维护了快照创建者的信息以及每个快照的相关信息等等。&#xA;版本控制系统的事实标准是 Git。&#xA;Git 的许多操作或者说命令看起来非常奇怪，但 Git 的底层设计与思想非常优雅，因此，从 Git 的数据模型开始学习 Git，自底向上，最后再学习 Git 的接口或者说命令，会比较容易让人理解 Git 的命令以及 Git 是如何操作数据模型的。&#xA;Git 的数据模型 Git 将项目的根目录（顶层目录）中的文件夹与文件作为集合，通过这一系列集合的快照来管理项目历史记录。在 Git 的术语中，文件被称为 Blob（数据对象），即一组数据，目录则是被称为 tree。tree 的名称与另一个 tree 又或者文件相对应。&#xA;把目录视为 tree，那么子目录就是 subtree，目录下的文件就是 tree 的子节点。&#xA;一棵 tree 看起来可能是这样的：&#xA;&amp;lt;root&amp;gt; (tree) | +- foo (tree) | | | + bar.txt (blob, contents = &amp;#34;hello world&amp;#34;) | +- baz.txt (blob, contents = &amp;#34;git is wonderful&amp;#34;) Git 历史记录建模：关联快照 Git 中的 object 可以分为 blob、tree、commit 三类，每次我们执行 git commit 时，都会创建一个 commit 对象，又或者说对当前的 work directory 的 snapshot。</description>
    </item>
    <item>
      <title>计算机教育缺失的一课：命令行环境</title>
      <link>http://localhost:1313/posts/tech/missing-semester-command-line.zh/</link>
      <pubDate>Sat, 15 Jun 2024 15:18:13 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/missing-semester-command-line.zh/</guid>
      <description>任务控制 信号与终止进程 shell 会使用 unix 提供的信号机制来进行进程之间的通信，当一个进程接收到信号时，它会停止执行原来的任务、处理该信号、并基于该信号传递的信息来改变任务的执行，可以认为信号是一种 软件中断。&#xA;下面这个 python 程序演示了捕获 SIGINT 信号并忽略该信号，即这个程序在收到 SIGINT 信号时，不会终止程序，我们需要使用 SIGQUIT 信号来停止这个程序，可以通过 &amp;lt;C-\&amp;gt; 来发送该信号。&#xA;#!/usr/bin/env python import signal, time def handler(signum, time): print(&amp;#34;\nI got a SIGINT, but I am not stopping&amp;#34;) signal.signal(signal.SIGINT, handler) i = 0 while True: time.sleep(.1) print(&amp;#34;\r{}&amp;#34;.format(i), end=&amp;#34;&amp;#34;) i += 1 运行该程序，向该程序发送两次 SIGINT，然后发送一次 SIGQUIT，程序反应如下：&#xA;zwyyy in 🌐 d3855u in ~/missing-semester 13s ❯ python3 sig.py 28^C I got a SIGINT, but I am not stopping 53^C I got a SIGINT, but I am not stopping 63^\zsh: quit python3 sig.</description>
    </item>
    <item>
      <title>计算机教育缺失的一课：数据整理</title>
      <link>http://localhost:1313/posts/tech/missing-semester-data-organize.zh/</link>
      <pubDate>Fri, 14 Jun 2024 19:26:09 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/missing-semester-data-organize.zh/</guid>
      <description>前言 您是否曾经有过这样的需求，将某种格式存储的数据转换成另外一种格式? 肯定有过，对吧！ 这也正是我们这节课所要讲授的主要内容。具体来讲，我们需要不断地对数据进行处理，直到得到我们想要的最终结果。&#xA;在之前的课程中，其实我们已经接触到了一些数据整理的基本技术。可以这么说，每当您使用管道运算符的时候，其实就是在进行某种形式的数据整理。&#xA;例如这样一条命令 journalctl | grep -i intel，它会找到所有包含intel(不区分大小写)的系统日志。您可能并不认为这是数据整理，但是它确实将某种形式的数据（全部系统日志）转换成了另外一种形式的数据（仅包含intel的日志）。大多数情况下，数据整理需要您能够明确哪些工具可以被用来达成特定数据整理的目的，并且明白如何组合使用这些工具。&#xA;让我们从头讲起。既然是学习数据整理，那有两样东西自然是必不可少的：用来整理的数据以及相关的应用场景。日志处理通常是一个比较典型的使用场景，因为我们经常需要在日志中查找某些信息，这种情况下通读日志是不现实的。现在，让我们研究一下系统日志，看看哪些用户曾经尝试过登录我们的服务器：&#xA;ssh myserver journalctl 内容太多了。现在让我们把涉及 sshd 的信息过滤出来：&#xA;ssh myserver journalctl | grep sshd 注意，这里我们使用管道将一个远程服务器上的文件传递给本机的 grep 程序！ ssh 太牛了，下一节课我们会讲授命令行环境，届时我们会详细讨论 ssh 的相关内容。此时我们打印出的内容，仍然比我们需要的要多得多，读起来也非常费劲。我们来改进一下：&#xA;ssh myserver &amp;#39;journalctl | grep sshd | grep &amp;#34;Disconnected from&amp;#34;&amp;#39; | less 多出来的引号是什么作用呢？这么说吧，我们的日志是一个非常大的文件，把这么大的文件流直接传输到我们本地的电脑上再进行过滤是对流量的一种浪费。因此我们采取另外一种方式，我们先在远端机器上过滤文本内容，然后再将结果传输到本机。 less 为我们创建来一个文件分页器，使我们可以通过翻页的方式浏览较长的文本。为了进一步节省流量，我们甚至可以将当前过滤出的日志保存到文件中，这样后续就不需要再次通过网络访问该文件了：&#xA;$ ssh myserver &amp;#39;journalctl | grep sshd | grep &amp;#34;Disconnected from&amp;#34;&amp;#39; &amp;gt; ssh.log $ less ssh.log 过滤结果中仍然包含不少没用的数据。我们有很多办法可以删除这些无用的数据，但是让我们先研究一下 sed 这个非常强大的工具。&#xA;sed 是一个基于文本编辑器ed构建的&amp;quot;流编辑器&amp;quot; 。在 sed 中，您基本上是利用一些简短的命令来修改文件，而不是直接操作文件的内容（尽管您也可以选择这样做）。相关的命令行非常多，但是最常用的是 s，即替换命令，例如我们可以这样写：</description>
    </item>
    <item>
      <title>计算机教育缺失的一课：编辑器（Vim）</title>
      <link>http://localhost:1313/posts/tech/missing-semester-editor-vim.zh/</link>
      <pubDate>Fri, 14 Jun 2024 16:55:19 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/missing-semester-editor-vim.zh/</guid>
      <description>编辑模式 Vim 的符号说明，对于 Ctlr+v 的组合键，可能有 ^V、Ctrl-v、&amp;lt;C-v&amp;gt; 三种表达方式。&#xA;Vim 的设计以大多数时间都花在阅读、浏览和进行少量编辑改动为基础，因此它具有多种操作模式：&#xA;正常模式：在文件中四处移动光标进行修改 插入模式：插入文本 替换模式：替换文本 可视化模式（一般，行，块）：选中文本块 命令模式：用于执行命令 正常模式即 normal 模式，我更习惯叫 normal 模式。&#xA;Vim 的模式：&#xA;normal &amp;lt;-&amp;gt; insert replace visual command-line 在 normal 模式下，按下 v 可以进入可视（一般）模式，按下 V 则可以进入可视（行）模式，而 &amp;lt;C-v&amp;gt; 则会进入可视（方块）模式。&#xA;buffer，window，tab vim 具有多个 tab（标签），每个 tab 可以包含多个 window，每个 window 对应一个 buffer，而同一个 buffer 可能由多个 window 打开。&#xA;:q 实际上只是关闭当前窗口，假设 vim 已经没有打开的窗口了，那么才会退出 vim。&#xA;命令模式 在 normal 模式下按下 : 可以进入命令模式，这个模式下，可以打开、保存、关闭文件，以及退出 Vim。&#xA;:q 退出（关闭窗口） :w 保存（写） :wq 保存然后退出 :e {文件名} 打开要编辑的文件 :ls 显示打开的缓存 :help {标题} 打开帮助文档 :help :w 打开 :w 命令的帮助文档 :help w 打开 w 移动的帮助文档 移动光标 在 normal 模式下，可以利用移动命令在 buffer 中移动光标，在 Vim 中，移动也被称为“名词”，与编辑命令（i、o）等相对应。</description>
    </item>
    <item>
      <title>Xv6 Lab11: Mmap</title>
      <link>http://localhost:1313/posts/tech/xv6-lab11.zh/</link>
      <pubDate>Thu, 03 Aug 2023 13:53:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab11.zh/</guid>
      <description>思路与实现 添加系统调用就不多说了。&#xA;整体流程应该是这样的，lab 的提示中，要求我们定义一个 vma 结构体，vma 的定义如下；然后 lab 的提示要求我们声明一个大小为 $16$ 的 vma 数组，并按需要从该数组分配，问题来了，数组在哪里声明呢？考虑到每个进程都有自己的虚拟地址空间，因此，每个进程都有自己的 virtual memory areas，要分配 vma 的时候，应该从每个进程自己的 vma 数组进行分配，于是，我们可以考虑为 struct proc 添加 struct vma areas[NVMA] 字段。&#xA;struct vma { int fd; int rw_flag; uint64 start; uint64 cur; uint len; int state; int flags; }; struct proc { // 已有的省略不写 struct vma areas[NVMA]; }; 在 vma 的定义中，start 表示起始地址，$[start, cur)$ 这一段虚拟地址（左闭右开）是已经绑定了 pp 的，pp 的数据与 file 绑定。&#xA;那么我们如何实现 sys_mmap 呢？这里可以参照 sbrk，递增 p-&amp;gt;sz，然后仿照 allocproc，寻找状态为 UNUSED 的 vma，分配给本次 sys_mmap。注意如果文件本身是 read_only，并且以 MAP_SHARED 模式进行 map，那么 flags 不能为 PROT_WRITE，write only 的情况同理（即文件不可读）。</description>
    </item>
    <item>
      <title>Xv6 Lab10: file system</title>
      <link>http://localhost:1313/posts/tech/xv6-lab10.zh/</link>
      <pubDate>Tue, 01 Aug 2023 20:19:47 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab10.zh/</guid>
      <description>Large files 这个作业需要我们将 xv6 的最大文件大小从 12 + 256 Bytes 修改为 11 + 256 + 256 * 256 Bytes。&#xA;为了达成这个目标，我们需要使用二级索引块，对 inode 的 addrs 字段，首先将 NDIRECT 从 $12$ 修改为 $11$，即前 $11$ 个 block 是 direct block，addrs[NDIRECT] 对应的块是一个一级索引块，这个块中的每个元素（共 BSIZE / sizeof(uint) 个元素）都是一个数据块的编号；而 addrs[NDIRECT + 1] 是一个二级索引块，这个块中的每个元素都是一级索引块的编号，由编号找到一级索引块，然后再由以及索引块找到数据块。&#xA;这个作业的主要任务就是修改 bmap 和 itrunc 两个函数。&#xA;先修改 fs.h 中的 NDIRECT 的相关定义：&#xA;#define FSMAGIC 0x10203040 #define NDIRECT 11 #define NINDIRECT (BSIZE / sizeof(uint)) #define NDINDIRECT (NINDIRECT * NINDIRECT) #define MAXFILE (NDIRECT + NINDIRECT + NDINDIRECT) struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT + 2]; // Data block addresses }; 注意 file.</description>
    </item>
    <item>
      <title>Xv6 Lab9: Locks</title>
      <link>http://localhost:1313/posts/tech/xv6-lab9.zh/</link>
      <pubDate>Sat, 29 Jul 2023 14:28:58 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab9.zh/</guid>
      <description>Memory allocator 这一题很简单，主要任务，就是为每个 cpu 维护一个空闲物理内存的链表 freelist，xv6 默认使用的结构体 kmem，其中包含一个 freelist 供所有的 cpu 使用。我们要做的，就是把 freelist 修改成 freelist 的数组，即 struct run *freelist[NCPU]，其中 NCPU 是定义于 kernel/params.h 的宏，对应 cpu 的个数。 kmem 中的 spinlock 是用来保护 freelist 的，既然 freelist 变成了数组，那么也需要有 NCPU 个 spinlock。因此，修改 kmem 为如下结构体：&#xA;struct { struct spinlock lock[NCPU]; struct run *freelist[NCPU]; // for each cpu, allocate a freelist } kmem; 接着，我们需要修改 kinit，让它初始化每个 spinlock。&#xA;void kinit() { char lockname[6] = {&amp;#39;k&amp;#39;, &amp;#39;m&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;m&amp;#39;, &amp;#39;0&amp;#39;, 0}; for (int i = 0; i &amp;lt; NCPU; ++i) { lockname[4] = &amp;#39;0&amp;#39; + i; initlock(kmem.</description>
    </item>
    <item>
      <title>MIT 6.S081 File system performance and fast crash recovery</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec16.zh/</link>
      <pubDate>Thu, 27 Jul 2023 16:34:51 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec16.zh/</guid>
      <description>引入 当我们针对文件系统讨论 logging 或者 journal 时，其实是在讨论同一件事，二者是同义词。&#xA;这一部分主要是讨论 Linux 的 ext3 文件系统，它相比 ext2，可以就说就是加了一层 logging，其他基本没有改变。我们要关注 ext3 与 xv6 的文件系统的不同之处，重点放在 ext3 是如何在保证 logging 的同时尽可能提升性能的。&#xA;ext3 file system log format ext3 数据结构与 xv6 相似，在内存中存在 block cache，它们是 write-back 的（即改动不会马上写回到磁盘）。&#xA;logging 系统有两个非常重要的准则：&#xA;write-adead rule：必须现在 log 中记录好所有这些写操作，才能将这些写操作应用到磁盘； freeing rule：即我们不能覆盖或者重用 log。 ext3 还维护了一些 transaction 的信息，transaction_t 中包含：&#xA;一个序列号； 一系列该 transaction 修改的 block 的编号，这些 block 编号指向 cache 中的 blcok； 一系列的 handle，handle 对应属于transaction 的系统调用，它们会读写 cache 中的 block； ext3 的磁盘组织与 xv6 类似，存在一个文件系统树，包含 inode、目录、file 等，存在 bimtap lock 来标识每个 data block 是被分配还是空闲的，在磁盘的一个指定区域保存 log。</description>
    </item>
    <item>
      <title>Mit6.s081 Lec15: xv6 的 logging system</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec15.zh/</link>
      <pubDate>Tue, 25 Jul 2023 16:48:39 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec15.zh/</guid>
      <description>Logging layer file system 设计的一大重要问题就是 crash recovery。这是因为文件系统操作往往涉及向磁盘多次写入，而几次写入之后的 crash 可能导致磁盘上的文件系统处于一个不一致的状态。&#xA;For example, suppose a crash occurs during file truncation (setting the length of a file to zero and freeing its content blocks). Depending on the order of the disk writes, the crash may either leave an inode with a reference to a content block that is marked free, or it may leave an allocated but unreferenced content block.&#xA;前者当系统重启之后,可能导致一个磁盘 block 被两个文件所对应，这是一个很严重的问题。</description>
    </item>
    <item>
      <title>MIT 6.S081 Lec14: File system</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec14.zh/</link>
      <pubDate>Sat, 22 Jul 2023 19:51:29 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec14.zh/</guid>
      <description>Overview 文件系统的设计目标就是组织和存储数据，文件系统一个比较重要功能是持久化，即重启之后，数据不会丢失。xv6 通过把数据存储在 virtio disk 上来实现持久化。&#xA;文件系统设计的几大挑战：&#xA;The file system needs on-disk data structures to represent the tree of named directories and files, to record the identities of the blocks that hold each file’s content, and to record which areas of the disk are free. 由于文件系统需要实现持久化，因此必须要实现 crash recovery，即如果发生意外的 crash（例如断电），文件系统在计算机重启之后要能依旧正常工作； 可能有多个进程同时操作文件系统； 由于访问磁盘比访问内存要慢得多得多，因此文件系统需要能支持将部分 popluar 的 blocks 缓存在内存中； xv6 的文件系统可以说组织为七层，如下图所示：&#xA;disk layer 负责读写 virtio hard drive 上的 blocks，buffer cache layer 是 blocks 的 cache，并且保证同一时间，只有一个内核进程可以修改存储在特定块上的数据；logging layer 将对几个特定 block 的更新打包为一次 transaction（就是数据库常说的事务？），从而确保这些 blocks 都是被原子化地更新，即要么一次都更新，要么一次都不更新；inode layer 则是用来表示单独的文件，每个文件都是以具有不重复的 index 的 inode 和保存了这个文件的数据的一些 blocks 来表示；而在 directory layer，每个 directory 都是一种特殊的 inode，包含一系列 direcotry entry，directory entry 则是包含了文件名和 index（对应 indode layer 所说的 index）；pathname layer 提供了层级化的路径名，利用递归查找来解析它们；</description>
    </item>
    <item>
      <title>Xv6 Lab7: Multithreading</title>
      <link>http://localhost:1313/posts/tech/xv6-lab7.zh/</link>
      <pubDate>Sat, 22 Jul 2023 11:30:01 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab7.zh/</guid>
      <description>Uthread: switching between threads 这个题还是对的起它 moderate 的难度了，如果认真看了 book-riscv-rev2.pdf 的 Scheduling 章节，以及看了这个 课程翻译，那么这题可以很快做出来，个人觉得 pdf 讲得更加清楚一些。&#xA;这个题甚至帮你把需要添加代码的地方都标注出来了，参照题目说明，主要有三步：&#xA;修改 thread_create 来保证当 thread_schedule 第一次运行 thread_create 创建出来的线程时，该线程就会在自己的 stack 上执行传递给 thread_create 的函数，这里我们可以参照 allocproc 的实现，在 thread_create 标记出来的要我们添加代码的地方添加如下三行： memset(&amp;amp;t-&amp;gt;context, 0, sizeof(t-&amp;gt;context)); t-&amp;gt;context.ra = (uint64)func; t-&amp;gt;context.sp = (uint64)t-&amp;gt;stack + STACK_SIZE; 保证 thread_switch 会切换并保存寄存器，这里参照 scheduler 的实现即可，在注释标记的地方添加以下语句，并且在 uthread_switch.S 中实现 thread_switch 函数（照抄 swtch 即可）： thread_switch((uint64)&amp;amp;t-&amp;gt;context, (uint64)&amp;amp;current_thread-&amp;gt;context); thread_switch: /* YOUR CODE HERE */ sd ra, 0(a0) sd sp, 8(a0) sd s0, 16(a0) sd s1, 24(a0) sd s2, 32(a0) sd s3, 40(a0) sd s4, 48(a0) sd s5, 56(a0) sd s6, 64(a0) sd s7, 72(a0) sd s8, 80(a0) sd s9, 88(a0) sd s10, 96(a0) sd s11, 104(a0) ld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1) ret /* return to ra */ 修改 strcut thread 来存储 thread_switch 时需要保存的寄存器，还是参照 struct proc 即可： struct t_context { uint64 ra; uint64 sp; // callee saved uint64 s0; uint64 s1; uint64 s2; uint64 s3; uint64 s4; uint64 s5; uint64 s6; uint64 s7; uint64 s8; uint64 s9; uint64 s10; uint64 s11; }; struct thread { char stack[STACK_SIZE]; /* the thread&amp;#39;s stack */ int state; /* FREE, RUNNING, RUNNABLE */ struct t_context context; }; 这样修改之后就能通过 uthread 了。</description>
    </item>
    <item>
      <title>MIT 6.S081 Sleep &amp; Wake up</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec13.zh/</link>
      <pubDate>Thu, 20 Jul 2023 17:40:26 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec13.zh/</guid>
      <description>Sleep and wakeup Sleep 允许一个内核线程等待某个特定事件的发生，另一个线程可以调用 wakeup 来表示这个正在等待时间发生的线程应该恢复了。&#xA;Sleep and wakeup are often called sequence cooridination or conditional synchronization mechanisms.&#xA;xv6 中利用 sleep 和 wakeup 实现了一种 high-level 的同步机制，被称为信号量（semaphore），用于协调生产者和消费者（xv6 中并未使用信号量）。&#xA;A semaphore maintains a count and provides two operations. The “V” operation (for the producer) increments the count. The “P” operation (for the consumer) waits until the count is non-zero, and then decrements it and returns.&#xA;struct semaphore { struct spinlock lock; int count; }; void V(struct semaphore *s) { acquire(&amp;amp;s-&amp;gt;lock); s-&amp;gt;count += 1; // wakeup(s); release(&amp;amp;s-&amp;gt;lock); } void P(struct *s) { while (s-&amp;gt;count == 0) //sleep(s); ; acquire(&amp;amp;s-&amp;gt;lock); s-&amp;gt;count -= 1; release(&amp;amp;s-&amp;gt;count); } 上述代码给出了一个非常简单但是性能不优秀的 “生产者-消费者” 模型实现，如果生产者很少工作，那么消费者会花费大量时间在 while 循环中。为了避免这一点，消费者应该要有办法主动让出 cpu，并且只在 V 递增 s-&amp;gt;count 之后才恢复执行。</description>
    </item>
    <item>
      <title>MIT 6.S081 Thread switching</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec11.zh/</link>
      <pubDate>Wed, 19 Jul 2023 10:46:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec11.zh/</guid>
      <description>Multiplexing xv6 通过将 cpu 从一个进程切换到另一个进程来实现 multiplex（多路复用），进程的切换会在两种情形下发生：&#xA;xv6 的 sleep 与 wakeup 机制在进程等待 IO 完成或者等待子进程退出又或者在 sleep 系统调用中等待的时候切换进程。 xv6 会周期性地强制切换进程，从而应对那些长时间切换而未 sleep 的进程。 这个 multiplex 机制会让进程产生一种自己完全拥有 cpu 的错觉，就像 xv6 用虚拟内存和 page table 机制让进程觉得自己拥有完整的内存空间一样。&#xA;xv6 使用硬件定时器中断来保证 context switch（上下文切换）。&#xA;Code: Context switching 用户进程之间的切换步骤如下图所示：&#xA;用户进程之间的切换其实会经过两次 context switch，以上图为例，第一次是从 shell 用户进程的 kernel thread 切换到 cpu 的 scheduler thread；第二次从 cpu 的 scheduler thread 切换到新用户进程（例如 cat）的 kernel thread。&#xA;在 xv6 中，我们可以认为每个用户进程，包含一个内核线程与一个用户线程，然后每个 cpu 包含一个 scheduler thread，schedular thread 工作在内核中，有只属于它的 kernel stack。&#xA;swtch 执行为内核线程切换的保存和恢复工作。swtch 的主要工作就是保存和恢复 riscv 的寄存器，又被称为上下文。</description>
    </item>
    <item>
      <title>MIT 6.S081 Multiprocessors and locking</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec10.zh/</link>
      <pubDate>Tue, 18 Jul 2023 13:54:16 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec10.zh/</guid>
      <description>why lock 防止多核并行运行下的 race condition 导致的错误。&#xA;内核中的数据是典型的 concurrently-accessed 的数据。&#xA;race condition and how the lock avoid it A race condition is a situation in which a memory location is accessed concurrently, and at least one access is a write.&#xA;Locks ensure mutual exclusion. 锁的持有与释放之间的语句会被原子化，在 xv6 中，就是 acquire 和 release 之间的多条语句，只能被一个进程（CPU）全部执行完了之后，才可能被其他的进程（CPU）执行，这样就避免了 race condition。&#xA;acquire 和 release 之间的多条指令通常被称为 critical section。&#xA;lock 在某种意义上是在维护 critical section 中一些数据的不变量（some collection of invariants），这个不变量在 critical section 中可能会被暂时破坏，但是当 critical section 的最开始，以及结束的时候，这个 invariants 一定成立！ 例如 kfree 中的 lock，就是在维护 kmem.</description>
    </item>
    <item>
      <title>MIT 6.S081 Page faults</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec08.zh/</link>
      <pubDate>Mon, 17 Jul 2023 16:27:16 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec08.zh/</guid>
      <description>概述 这一章主要聚焦于，我们利用 virtural memory 和 page fault 这两个机制，能够实现一些什么样的有意思的优化。&#xA;虚拟内存的有两大优势：&#xA;Isolation，保证每个进程都有它自己的虚拟地址空间，写自己的虚拟地址处的数据不会破坏其他进程的数据； Levle of indirection，提供了一层抽象（这里不是很好理解），可以理解为提供了一层从虚拟地址到物理地址的映射关系，利用这个映射关系，我们可以实现很多有意思的优化。 利用 page fault，我们可以更新 page table，即更改虚拟地址和物理地址之间的映射关系（在之前的 xv6 中，可以说 va 和 pa 的映射关系，在进程启动之后，到进程结束之前，都是固定的）。&#xA;对于 page fault，也可以说是一种 trap，之前提到的 system call 是发生了系统调用之后的 trap，因此 trap 完成之后，我们会返回到产生系统调用的指令的下一条指令继续执行；而 page fault 则是异常（exception）导致的 trap，trap 结束之后，我们会返回导致 page fault 的指令，重新执行这一条指令；&#xA;正如 system call 导致的 trap 中，我们需要实现真正执行 systemcall 的函数；而 page fault 导致的 trap 中，我们也需要处理这一异常（一般是在 trap.c 的 usertrap 函数中）。&#xA;对于处理 page fautl 的思路，其实可以参照 system call，我们通过读取 scause 寄存器的值来判断导致 trap 的原因，如果是 $13$ 或者 $15$，则说明是 page fault。</description>
    </item>
    <item>
      <title>Xv6 Lab6: Copy-on-Write Fork for xv6</title>
      <link>http://localhost:1313/posts/tech/xv6-lab6.zh/</link>
      <pubDate>Mon, 17 Jul 2023 13:42:27 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab6.zh/</guid>
      <description>思路 经过 lab5: lazy page allocation 之后，对 xv6 的 page fault 的处理，算是有所了解了。&#xA;今天这个 COW 实验，在 2020 年的课程视频中有对思路的讲解，可以先看看 课程翻译，厘清一下思路。&#xA;整体思路其实也不难，默认情况下，fork 会调用 uvmcopy，将父进程的 PP（物理页）复制一份，将这个 PP 的副本映射到子进程的 pagetable 的 VP（虚拟页）（子进程和父进程具有相同的虚拟地址，不同的 pagetable，不同的 PP，但是相同虚拟地址对应的 PP 的内容是一样的）。&#xA;我们这里讨论 vaddr、paddr 都是基于地址已经是 PGSIZE 对齐的情况来讨论的。&#xA;我们要做的修改就是，不再复制这个 PP，而是将 PP 的 paddr 同时映射到父进程的 vaddr 以及子进程的 vaddr。&#xA;在未修改 uvmcopy 之前，修改父进程的 vaddr 处的内容并不会影响子进程的 vaddr 处的内容，因为两个 vaddr 对应的是不同的 PP，只是 PP 的内容相同而已（在 pp 是 clean 的情况下）。&#xA;而修改了 uvmcopy 之后，写入父进程的 vaddr 会影响子进程的 vaddr 处的内容，这是我们不希望看到的，因此我们将这个 PP 对应的父进程的 pte 和子进程的 pte 的 PTE_W 位都清零，即标记为不可写，这样，当我们试图往这个 PP 中写入内容的时候，就会出现 page fault。</description>
    </item>
    <item>
      <title>Xv6 Lab5: lazy page allocation</title>
      <link>http://localhost:1313/posts/tech/xv6-lab5.zh/</link>
      <pubDate>Sat, 15 Jul 2023 17:18:30 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab5.zh/</guid>
      <description>前言 这个实验只有 2020 年的才有，2021 年的课程中是没有的，但是感觉这个实验还是挺有意义的，因此用 docker 创建了一个 debian 12 的容器，在容器中搭建了 2020 的实验环境，实验环境的搭建过程可以参照 MIT 6.s081 实验环境搭建。&#xA;Eliminate allocation from sbrk() 这个算是最简单的：&#xA;// kernel/sysproc.c uint64 sys_sbrk(void) { int addr; int n; if (argint(0, &amp;amp;n) &amp;lt; 0) { return -1; } addr = myproc-&amp;gt;sz; myproc()-&amp;gt;sz += n; // 添加的部分，修改 p-&amp;gt;sz，然后注释掉下面这三行 // if (growproc(n) &amp;lt; 0) { // return -1; // } return addr; } Lazy allocation 在去掉了 sys_sbrk 的 growproc 部分之后，由于只是单纯增加了 p-&amp;gt;sz，而没有给对应的虚拟地址分配物理页，因此，在执行 echo hi 时，会访问到 heap 中的未分配物理页的虚拟地址，于是出现 page fault，默认的 Xv6 的代码中并没有给出对 page fault 的处理，而是会直接杀死进程，因此无法正常执行完 echo hi。</description>
    </item>
    <item>
      <title>Xv6 Lab4: Traps</title>
      <link>http://localhost:1313/posts/tech/xv6-lab4.zh/</link>
      <pubDate>Thu, 13 Jul 2023 12:51:50 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab4.zh/</guid>
      <description>RISC-V assembly Which registers contain arguments to functions? For example, which register holds 13 in main&amp;rsquo;s call to printf?&#xA;a2 寄存器，函数调用时，参数从左到右会依次保存在 a0, a1, a2, a3 寄存器，似乎是一直到寄存器 a7 的。 Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)&#xA;这里的调用都被内联了 At what address is the function printf located?&#xA;auipc（Add Upper Immediate PC）指令是将一个立即数左移 $12$ 位加上当前指令的地址（pc）中，得到一个绝对地址。&#xA;例如 auipc a0, 0x0 就是将 $\text{0x0}$ 左移 $12$ 位加上当前指令的地址 (pc) 中（pc 的值我们一般认为是在指令执行完成时才发生递增，从而指向下一条指令，使得处理器能够按顺序顺利执行指令序列），因此，执行 auipc a0, 0x0 时，加的就是当前指令的地址，即 $\text{0x28}$。</description>
    </item>
    <item>
      <title>MIT 6.S081 Isolation &amp; System call entry/exit</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-lec06.zh/</link>
      <pubDate>Sat, 08 Jul 2023 15:32:43 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-lec06.zh/</guid>
      <description>Trap 机制 程序运行往往需要完成用户空间和内核空间的切换，每当：&#xA;程序执行系统调用（system call）； 程序出现了 page fault 等错误； 一个设备触发了中断； 都会发生这样的切换。&#xA;这里用户空间切换到内核空间通常被称为 trap，因此有时候我们会说程序“陷入”到内核态。trap 机制需要尽可能的简单。&#xA;trap 的工作，可以说是让硬件从适合运行用户程序的状态，切换到适合运行内核代码的状态。&#xA;这里说的状态中，我们最关心的状态可能是 $32$ 个用户寄存器，我们尤其需要关注以下硬件寄存器的内容：&#xA;堆栈寄存器（stack register，又称 stack pointer）； 程序计数器（Program Counter Register）； 表明当前 mode 的标志位的寄存器，表明当前是 supervisor mode 还是 user mode； 控制 CPU 工作方式的寄存器，例如 SATP（Supervisor Address Translation and Protection）寄存器，它包含了指向 page table 的物理内存地址； STVEC（Supervisor Trap Vector Base Address Register）寄存器，它指向了内核中处理 trap 的指令的起始地址； SEPC（Supervisor Exception Program Counter）寄存器，在 trap 的过程中保存程序计数器的值； sscratch（Supervisor Scratch Register）寄存器； 在 trap 的最开始，CPU 所有的状态肯定还是在运行用户代码而不是内核代码，在 trap 处理的过程中，我们会逐渐更改状态，或者对状态做一些操作，我们可以设想一下我们需要做哪些操作：&#xA;保存 $32$ 个用户寄存器的状态，例如，当响应中断完成后，我们会希望能恢复用户程序的执行，而这些寄存器需要被内核代码所使用，因此，在 trap 之前，我们需要保存这 $32$ 个用户寄存器的内容； 保存 PC 的内容，原因类似于保存 $32$ 个用户寄存器； 将 mode 修改为 supervisor mode； 运行内核代码前，将 SATP 由指向 user page table 修改为指向 kernel page table； trap 机制不会依赖于 $32$ 个用户寄存器；</description>
    </item>
    <item>
      <title>MIT 6.S081 页表</title>
      <link>http://localhost:1313/posts/tech/mit6.s081-page-table.zh/</link>
      <pubDate>Mon, 03 Jul 2023 09:19:09 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/mit6.s081-page-table.zh/</guid>
      <description>Paging hardware 总的来说，Xv6 的虚拟内存到物理内存的映射方式与 x64 是一致的，都是使用页表来进行映射。区别在于，Xv6 只使用了三级页表，而 x64 则是使用四级页表，另外，二者的页表层级的命名也有区别，对 Xv6 来说，最高级的页表是 L3（其地址存放于寄存器 satp 中）。&#xA;每个 page table 含有 512 个 page table entry，而每个 page table entry 的大小是 8KB，因此一个 page table 占据的大小正好是 4KB，即一个 VP 的大小！&#xA;标志位可以说是顾名思义，除了这个 dirty？（留待之后处理）&#xA;Kernel address space Xv6 中，每个进程都有属于自己的地址空间，以及一个全局唯一的描述内核地址空间的 page table，（kernel page table）。&#xA;内核内存布局：&#xA;QEMU 模拟了一台带 RAM（物理内存）的电脑，该 RAM 的起始地址是 $\text{0x80000000}$ ,结束地址至少是 $\text{0x86400000}$（该地址在 Xv6 中被定义为 PHYSTOP），这里的地址说的都是物理地址。&#xA;QEMU 会将设备接口以内存映射的控制寄存器暴露给系统软件，这些寄存器地址映射的内存都是在 $\text{0x80000000}$，即系统要访问这些设备，都是通过 $\text{0x80000000}$ 以下的物理地址直接访问，但通过这样的物理地址访问设备就不会经过 RAM 了。&#xA;内核空间的虚拟地址是直接映射到物理地址的，例如 KERNBASE=0x80000000，虚拟地址和物理地址都是这个值，可以理解为虚拟地址等于物理地址。&#xA;但是有几个内核虚拟地址不是直接映射的，如下图所示：&#xA;trampoline page（蹦床页面），它映射在虚拟地址空间的顶部，user page table 具有相同的映射，即不论 kernel page table 还是 user page table，trampoline page 的虚拟地址都是在虚拟地址空间的顶部； kernel stack page，每个进程都有自己的 kernel stack，会映射到虚拟地址空间中比较高的那个 kernel stack，这样可以利用到 kernel stack 下方的那个 guard page。Guard page is invalid!</description>
    </item>
    <item>
      <title>Xv6 Lab2: System calls</title>
      <link>http://localhost:1313/posts/tech/xv6-lab2.zh/</link>
      <pubDate>Sat, 01 Jul 2023 15:15:38 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-lab2.zh/</guid>
      <description>系统调用 Lab1 主要是基于提供的系统调用接口来编写一些小工具程序，而 Lab2 则是要我们自己实现系统调用，并提供系统调用的接口。&#xA;以本次 Lab 要我们实现的 trace 调用为例，说明一下系统调用的流程：&#xA;在 user/trace.c 的第 $15$ 行，调用了属于 system call 的 trace 函数，当前执行 make qemu 是无法成功的，因为我们还没有给用户提供接口。因此，我们需要在 user/user.h 里面添加系统调用 trace 的函数声明（prototype）。&#xA;我们需要在 user/usys.pl 中追加 entry(&amp;quot;trace&amp;quot;); 这一行，从而添加一个 trap entry，从而实现调用 trace 时会发生 trap，从而会执行 ECALL 指令，并且会将系统调用的接口的参数（这里就是 trace 的参数）的存入寄存器 a0、a1 等（从左往右第一个参数的地址存入 a0，依次类推），此外，还会将 trace 对应的系统调用号存入寄存器 a7。&#xA;之后控制权来到 kernel 中的 syscall 函数，它从 a7 中取出系统调用号，并执行系统调用号对应的 sys_func。&#xA;这里的系统调用号可以理解为数组索引，在本次 Lab 中需要我们修改 kernel/syscall.c 和 kenel/syscall.h 从而添加 trace 对应的系统调用号，以及内核中 trace 对应的 sys_trace 的实现）。&#xA;System call tracing 官网的提示其实是比较全面了，按提示处理，即可添加 trace 的系统调用接口：</description>
    </item>
    <item>
      <title>MIT 6.S081 操作系统组织架构</title>
      <link>http://localhost:1313/posts/tech/xv6-os-organization.zh/</link>
      <pubDate>Thu, 29 Jun 2023 18:59:04 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/xv6-os-organization.zh/</guid>
      <description>进程概述 64 位的 RISC-V 的 VAS 是 39 位的，即 VA 只有 39 位，而 Xv6 则只有 38 位，最大虚拟地址为 #define MAXVA 0x3fffffffff。&#xA;VAS 的顶端，即最高位存放了两个 page，一个是用于 trampoline，一个用于 mapping the process&amp;rsquo;s trapframe。 Xv6 使用这两个 page 来切换到内核以及返回。&#xA;进程的状态被定义在 kernel/proc.h 的结构体 struct proc 所描述，进程在内核中最重要的信息就是它的 page table、 kernel stack 以及运行状态（run state）。&#xA;A process can make a system call by executing the RISC-V ecall instruction. This instruction raises the hardware privilege level and changes the program counter to a kernel-defined entry point.</description>
    </item>
    <item>
      <title>Math_test</title>
      <link>http://localhost:1313/posts/tech/math_test/</link>
      <pubDate>Mon, 26 Sep 2022 11:40:26 +0800</pubDate>
      <guid>http://localhost:1313/posts/tech/math_test/</guid>
      <description>math_test 中文测试 $a_b$ $$a_b + c_d$$&#xA;aaa</description>
    </item>
  </channel>
</rss>
